[TOC]

# 9. 深入理解Binder

首先要说清楚的是，本次学习的是native层的Binder，而不是java层的，后面会再讲java层的，所以下面讲到的进程通信都是native之间的进程进行通信，而不是java层的。

# Binder通信的实现原理

每个进程在运行时系统都会把各个进程隔离，每个进程都会被分到一块内存，以便用来运行程序。在linux上，不同的进程可能会运行在不同的空间中：有的进程在内核空间，有的在用户空间。那Binder跨进程通信是如何实现的呢？就是通过Binder驱动实现的，而这个Binder驱动运行在内核空间，每个进程只需要向该驱动打开设备，并且通过mmap映射内存提供用户空间进程和内核的数据交换速率，通过ioctl方式与内核进行数据交互，这样BInder驱动就作为一个中间者进行两个进程之间的传话者。

Binder的架构是属于c/s架构，所以会有客户端去找服务端的过程，这就类似于打电话：

在很久以前，打电话是这样的：

~~~java
+ 你打电话给服务中心，请帮我联系一下A。
+ 服务中心就帮你接通了A
+ 然后你和A就能愉快的通话了。
~~~

对应基于Binder的进程通讯这是不是这样呢：

~~~java
+ 某个客户端打电话给Binder，请帮我联系一下A服务端。
+ Binder就帮你接通了A
+ 然后你和A就能愉快的通话了。
~~~

是这样吧？这里服务中心是指的是binder，你仔细想想，他负责了帮客户端去找到服务端，然后还要为他们提供”通话服务“！！任务会不会重了一点？

那现代的打电话过程是怎样的呢？

~~~java
+ 你查找通讯录，然后拨通了A的电话号码
+ 服务中心为你接通A
+ 你和A愉快通讯
~~~

你一看，嗯？多了个查找通讯录的过程，你知道这个号码对应的是A，服务中心看你拨打的电话后也知道他是A，就帮你接通了。基于Binder通信是这样的

~~~java
+ 某个客户端要找A服务端
+ 向通讯录通信！我要A服务端
+ 通讯录给你了A服务的代理对象
+ 你和A服务端通过服务中心提供的通信功能在愉快的聊天
~~~

所以说后面这种才是符合现实生活中的逻辑！BInder只负责通信，他才不负责帮你查找别的服务端！所以说负责查找的功能是由这个通讯录实现的吧！所以每一个服务端都要向这个通讯录进行注册吧？那是不是要找到一个服务端，然后查看他是怎么去向通讯录注册的？

# MediaServer

首先要先明白这是个native的服务端，而不是我们平时开发所用到的service，他是和zygote是一个类型的，都是main类型的服务。负责为安卓搞定linux的多媒体的东西。我们看看他是什么时候会启动

~~~java
service zygote /system/bin/app_process64 -Xzygote /system/bin --zygote --start-system-server
    class main
    socket zygote stream 660 root system
    onrestart write /sys/android_power/request_state wake
    onrestart write /sys/power/state on
    onrestart restart audioserver
    onrestart restart cameraserver
    onrestart restart media //就是这个！！在zygote启动的时候会去重新启动。
    onrestart restart netd
    writepid /dev/cpuset/foreground/tasks

~~~

你可能会疑惑，前面不是说zygote留了个socket吗？这不就是可以进程通信？但是需要注意的是，我们说的是他是留给他的 “子孙后代” 联系他的，从上面可以看出来MediaSercive是和zygote进程是同一个级别的哦！！所以这个Media进程和zygote进程进行通信是跨进程的，他们使用的是Binder！所以我们没有找错。

我们再想一个问题：这个所谓向的通讯录注册的过程，是不是也是需要进程通信呢？是的！！！

看看这个服务端的初始化过程吧！注意！这个MediaServer对于“通讯录”来说，就是客户端！！

~~~java
//media/ba/sd/aosp/frameworks/av/media/mediaserver/main_mediaserver.cpp


int main(int argc __unused, char **argv __unused)
{
    signal(SIGPIPE, SIG_IGN);

    //这个ProcessState可是非常的重要，等下会讲
    sp<ProcessState> proc(ProcessState::self());
    //获取了一个IServiceManager
    sp<IServiceManager> sm(defaultServiceManager());
   ...省略
}
~~~

每一个进程都有一个ProcessState，他负责保存这个进程的信息，和进行一些初始化操作，所以应该是一开始就创建他。在上面的代码中拿到了一个IServiceManager对象，但是我们一直说的通讯录其实就是ServiceManager。

为什么是IServiceManager而不是ServiceManager呢？这就涉及到Binder通信的过程了，客户端和服务端通信的过程中，两个进程都不会拿到对方的真正实例，而是一个代理接口对象，里面只有具体实现的函数，但是函数都没有具体的实现，所以从这里我们可以推测出BInder的通信过程：



+ 客户端 class A 里面定义了一堆方法，然后又定义了一个interface IA，里面也定义了相同的方法，比如

~~~c

class A{
	public void method1(){
		//省略一堆代码
	}
	public void method2(){
		//省略一堆代码
	}
}

Interface IA{
	public void method1(){}
	public void method2(){}
}

~~~

+ 然后服务端也是定义了一个代理类IServiceManager，在这里，服务端是serviceManager。
+ 现在客户端的目的是联系到ServiceManager这个服务端，告诉他，老子要在你这里注册！！
+ 怎么联系呢？通过ServiceManager提供的接口IServiceManager
+ 客户端拿到这个接口后，调用该接口定义的方法，注意！这里调用的是空方法，但是！BInder驱动知道你要调用的是ServiceManager的哪个方法，会帮你去调用serviceManager的方法，然后把结果返回给你！！(这个过程其实非常的复杂，后面会讲）所以说，客户端并没有真正的去操作到serviceManager，而是Binder驱动去帮他操作的

下面我们就要开始证实我们的推测！使用驱动我们都知道需要去驱动打开设备，那在哪呢？这个操作也是需要进程一被初始化就进行的，所以应该是在ProcessState初始化的时候，看看代码就知道了

~~~c
//media/ba/sd/aosp/frameworks/native/libs/binder/ProcessState.cpp
ProcessState::ProcessState()
    : mDriverFD(open_driver()) //哦！在这里，就去打开了Binder设备
    , mVMStart(MAP_FAILED) 
    , mThreadCountLock(PTHREAD_MUTEX_INITIALIZER)
    , mThreadCountDecrement(PTHREAD_COND_INITIALIZER)
    , mExecutingThreadsCount(0)
    , mMaxThreads(DEFAULT_MAX_BINDER_THREADS)
    , mStarvationStartTimeMs(0)
    , mManagesContexts(false)
    , mBinderContextCheckFunc(NULL)
    , mBinderContextUserData(NULL)
    , mThreadPoolStarted(false)
    , mThreadPoolSeq(1)
{
    if (mDriverFD >= 0) {
        // mmap the binder, providing a chunk of virtual address space to receive transactions.mmap操作提供了一种机制，让用户程序直接访问设备内存，这种机制，相比较在用户空间和内核空间互相拷贝数据，效率更高。这里就是前面说的每个进程其实都是和驱动共享了一部分内存，这样这些进程和驱动的数据交互会变得快速和方便，这其实是一个很重要的方法
        mVMStart = mmap(0, BINDER_VM_SIZE, PROT_READ, MAP_PRIVATE | MAP_NORESERVE, mDriverFD, 0);
        if (mVMStart == MAP_FAILED) {
            // *sigh*
            ALOGE("Using /dev/binder failed: unable to mmap transaction memory.\n");
            close(mDriverFD);
            mDriverFD = -1;
        }
    }

    LOG_ALWAYS_FATAL_IF(mDriverFD < 0, "Binder driver could not be opened.  Terminating.");
}


static int open_driver()
{
    //很熟悉的代码吧！打开后还进行了一些配置
    int fd = open("/dev/binder", O_RDWR | O_CLOEXEC);
    if (fd >= 0) {
        int vers = 0;
        status_t result = ioctl(fd, BINDER_VERSION, &vers);
        if (result == -1) {
            ALOGE("Binder ioctl to obtain version failed: %s", strerror(errno));
            close(fd);
            fd = -1;
        }
        if (result != 0 || vers != BINDER_CURRENT_PROTOCOL_VERSION) {
            ALOGE("Binder driver protocol does not match user space protocol!");
            close(fd);
            fd = -1;
        }
        //设置了fd支持的最大线程数是15
        size_t maxThreads = DEFAULT_MAX_BINDER_THREADS;
        result = ioctl(fd, BINDER_SET_MAX_THREADS, &maxThreads);
        if (result == -1) {
            ALOGE("Binder ioctl to set max threads failed: %s", strerror(errno));
        }
    } else {
        ALOGW("Opening '/dev/binder' failed: %s\n", strerror(errno));
    }
    return fd;
}
~~~

现在我们知道了该进程在接下来和其他进程的通信都是经过BInder驱动！而且和驱动还共享了一部分内存用于数据交换。上面我们说了客户端拿到了IServiceManager的对象后，会调用他的方法进而调用到ServiceManager的方法，但是其实是BInder驱动去帮你调用，然后返回结果的，那这些调用过程的数据是怎么传输的呢？就是通过共享的fd！！！

在上面的Main方法中我已经把使用IServiceManager的代码给省略了，因为现在还未到分析他的时候，还有一些疑问“

+ IServiceManager是怎么获取到的？BInder驱动给的吗？
+ Binder驱动又是怎么知道我们要调用的是ServiceManager服务端？
+ IServiceManager不就是个接口吗？他是怎么和BInder驱动进行通信的？

我们继续看下去

~~~c
//media/ba/sd/aosp/frameworks/av/media/mediaserver/main_mediaserver.cpp


int main(int argc __unused, char **argv __unused)
{
    signal(SIGPIPE, SIG_IGN);
    sp<ProcessState> proc(ProcessState::self());
    //获取了一个IServiceManager，我们下面仔细分析这个defaultServiceManager（）
    sp<IServiceManager> sm(defaultServiceManager());
   ...
}



~~~

~~~c
//media/ba/sd/aosp/frameworks/native/libs/binder/IServiceManager.cpp
sp<IServiceManager> defaultServiceManager()
{
    if (gDefaultServiceManager != NULL) return gDefaultServiceManager;

    {
        AutoMutex _l(gDefaultServiceManagerLock);
        while (gDefaultServiceManager == NULL) {
            //这里就是去实例化ServiceMananger的方法，可以看到是ProcessState提供的方法
            gDefaultServiceManager = interface_cast<IServiceManager>(
                ProcessState::self()->getContextObject(NULL));
            if (gDefaultServiceManager == NULL)
                sleep(1);
        }
    }

    return gDefaultServiceManager;
}
~~~



~~~c
//media/ba/sd/aosp/frameworks/native/libs/binder/ProcessState.cpp
sp<IBinder> ProcessState::getContextObject(const sp<IBinder>& /*caller*/)
{
    return getStrongProxyForHandle(0);
}

~~~

这里可以看到他传了个0？？而且看这个方法的名字：getStrongProxyForHandle，getStrongProxy我们是可以理解的：就是获取一个代理(这也证实了我们前面说的，和服务端进行进程通信时，是没有获取到他的具体实例的！)，ForHandle？？？Handle在window系统中就是fd的意思，所以这里Handle代表是值为0的fd，fd代表进程表中某个文件的下标值，那这个Handle为0是否就代表了serviceManager在BInder的 “电话号码” 呢？是的！！！所以现在我们明白了BInder驱动如何知道我们要找的是ServiceManager !

但是还有其他的疑问

+ IServiceManager是怎么获取到的？BInder驱动给的吗？
+ IServiceManager不就是个接口吗？他是怎么和BInder驱动进行通信的？

~~~c
//media/ba/sd/aosp/frameworks/native/libs/binder/ProcessState.cpp
sp<IBinder> ProcessState::getStrongProxyForHandle(int32_t handle)
{
    sp<IBinder> result;

    AutoMutex _l(mLock);

    //根据索引值查找，这里就是刚刚存进来的0.说明这里是为了不要多次获取servicemanager
    handle_entry* e = lookupHandleLocked(handle);

    if (e != NULL) {
        //如果没有在该进程找到serviceManager，说明第一次启动该进程，我们要和BInder沟通，拿到serviceManager的代理对象，可以看到这里多了个IBInder类型，然后看到return的是result，而result=b，说明servicemanager是IBinder类型的
        IBinder* b = e->binder;
        if (b == NULL || !e->refs->attemptIncWeak(this)) {
           ...

            //看这里！BpBinder！！！是什么东西？
            b = new BpBinder(handle); 
            e->binder = b; //把new出来的BpBinder放在e中。估计e就是个全局变量而已，有其他的作用，但我们这里不深究
            if (b) e->refs = b->getWeakRefs();
            result = b;
        } else {
            // This little bit of nastyness is to allow us to add a primary
            // reference to the remote proxy when this team doesn't have one
            // but another team is sending the handle to us.
            result.force_set(b);
            e->refs->decWeak(this);
        }
    }

    return result;
}
~~~

就new BpBinder(handle); 就完事了？？而且也没有看到有去操作BInder驱动吧！难道在BpBinder的构造函数？

~~~c
//media/ba/sd/aosp/frameworks/native/libs/binder/BpBinder.cpp
BpBinder::BpBinder(int32_t handle)
    : mHandle(handle)
    , mAlive(1)
    , mObitsSent(0)
    , mObituaries(NULL)
{
    ALOGV("Creating BpBinder %p handle %d\n", this, mHandle);

    extendObjectLifetime(OBJECT_LIFETIME_WEAK);
    //这个IPCThreadState很重要，但是我们先不奖罚
    IPCThreadState::self()->incWeakHandle(handle);
}

~~~

可以看到构造函数也没有去和BInder驱动交流。但是！这个BpBinder可是非常的重要，后面我们再说他。

再问自己：上面的IServiceManager是IBInder类型？不是的，我们看看IBinder的族谱，

![image](https://img2018.cnblogs.com/blog/570699/201810/570699-20181005004227764-912677979.png)

仔细找到IBinder的子类和父类，根本就没有出现过IServiceManager！所以说，IServiceManager和IBinder，没有任何的继承关系！但是在上面的代码中是我们要获取的是IBinder！不是IServiceManager，难道我们想错了？再回到上层的方法

~~~c
//media/ba/sd/aosp/frameworks/native/libs/binder/IServiceManager.cpp
sp<IServiceManager> defaultServiceManager()
{
    if (gDefaultServiceManager != NULL) return gDefaultServiceManager;

    {
        AutoMutex _l(gDefaultServiceManagerLock);
        while (gDefaultServiceManager == NULL) {
            //这里就是去实例化ServiceMananger的方法，可以看到是ProcessState提供的方法
            gDefaultServiceManager = interface_cast<IServiceManager>(
                ProcessState::self()->getContextObject(NULL));
            if (gDefaultServiceManager == NULL)
                sleep(1);
        }
    }

    return gDefaultServiceManager;
}
~~~

可以看到这里还走了个interface_cast方法！而且指定类型为IServiceManager，我们看看这个interface_cast里面到底做了什么

~~~c
//media/ba/sd/aosp/frameworks/native/include/binder/IInterface.h
template<typename INTERFACE>
inline sp<INTERFACE> interface_cast(const sp<IBinder>& obj)
{
    return INTERFACE::asInterface(obj);
}


~~~

啊！又要回到IServiceManager里面去看，因为调用的是泛型指定对象的方法

~~~c
//media/ba/sd/aosp/frameworks/native/include/binder/IServiceManager.h
class IServiceManager : public IInterface
{
public:
	//这里！！这个宏非常的重要，他的具体定义在下面会说
    DECLARE_META_INTERFACE(ServiceManager);
    ...
    
    
    //这个宏，是上面那个宏的具体实现，什么意思呢？看下去你就知道了
	IMPLEMENT_META_INTERFACE(ServiceManager, "android.os.IServiceManager");
}
~~~

~~~c
//media/ba/sd/aosp/frameworks/native/include/binder/IInterface.h
#define DECLARE_META_INTERFACE(INTERFACE)                               
    static const android::String16 descriptor;   
	//找的好苦啊，在这里！
    static android::sp<I##INTERFACE> asInterface(                       
            const android::sp<android::IBinder>& obj);                  
    virtual const android::String16& getInterfaceDescriptor() const;    
    I##INTERFACE();                                                     
    virtual ~I##INTERFACE();                                            
~~~

很明显，只是定义函数，没有具体的实现，因为具体实现在IMPLEMENT_META_INTERFACE这个宏，看看具体的实现

~~~c
//media/ba/sd/aosp/frameworks/native/include/binder/IInterface.h
#define IMPLEMENT_META_INTERFACE(INTERFACE, NAME) 
	//这里返回了一个字符串，用来描述一些东西的，外部传进来的
    const android::String16 I##INTERFACE::descriptor(NAME);          
    const android::String16&                                         
            I##INTERFACE::getInterfaceDescriptor() const {           
        return I##INTERFACE::descriptor;                             
    }    
	//我们看这里，原来是把前面new出来的BpBinder放在IServiceManager里面了
    android::sp<I##INTERFACE> I##INTERFACE::asInterface(             
            const android::sp<android::IBinder>& obj)                
    {                                                                
        android::sp<I##INTERFACE> intr;                              
        if (obj != NULL) {                                           
            intr = static_cast<I##INTERFACE*>(                       
                obj->queryLocalInterface(                            
                        I##INTERFACE::descriptor).get());            
            if (intr == NULL) {        
            	//重点在这里，##INTERFACE这个非常神奇，按照当前情景对应的是：BpServiceManager，obj就是前面生成的BpBInder实例 他把BpBInder传进去，然后返回了
                intr = new Bp##INTERFACE(obj);                         
            }                                                          
        }                                                              
        return intr;                                                   
    }                                                                  
    I##INTERFACE::I##INTERFACE() { }                                   
    I##INTERFACE::~I##INTERFACE() { }                                  


~~~

现在我们可以看出来，IServiceManager是把前面生成的BpBinder作为自己的一个字段，而BpBinder是属于IBinder的子类，所以说这里并没有把IServiceManager转为IBinder，谷歌工程师可真鸡贼，到现在我们都没有看到该进程去和BInder进行通信，只是实例化了一个包含了BpBinder实例的IServiceManager，而这个IServiceManager的具体类型其实是BpServiceManager！！嗯！IServiceManager是我们自己new出来的，我还以为是Binder驱动给的呢！

现在疑问就剩下

+ IServiceManager不就是个接口吗？他是怎么和BInder驱动进行通信的？

我们需要先了解一些类：

![image](https://wiki.jikexueyuan.com/project/deep-android-v1/images/chapter6/image002.png)

BpBinder和BBinder都是Android中与Binder通信相关的代表，它们都从IBinder类中派生而来，

+ BpBinder是客户端用来与Server交互的代理类，p即Proxy的意思。

+ BBinder则是proxy相对的一端，它是proxy交互的目的端。如果说Proxy代表客户端，那么BBinder则代表服务端。这里的BpBinder和BBinder是一一对应的，即某个BpBinder只能和对应的BBinder交互。我们当然不希望通过BpBinderA发送的请求，却由BBinderB来处理。

刚才我们在defaultServiceManager()函数中创建了这个BpBinder。这里有两个问题：

+ 为什么创建的不是BBinder？

因为MediaServer是ServiceManager的客户端，当然得使用代理端以与ServiceManager交互了。

ok！！！我们现在重新理一下：

+ 我们实例化了IServiceManager，这个是ServiceManager的一个代理端，为什么不是Binder驱动给呢？设想一下，如果我们后面加了很多类似ServiceManager这样的Server，而BInder驱动直接提供这些实例，那是不是每加一种Server，BInder驱动就要重新写？
+ 那既然是我们直接实例化一个服务端的代理，然后再和BInder通信，那Binder怎么知道我们这个代理对应的是哪个服务端？我们发现通信层就是由BpBinder和BBinder负责的，并且是一一对应的，而且实例化的时候给实例化BpBInder传了个handle值，这个值前面也有提到，他就是用来标识是哪个服务端的！！！而服务端的代理真正进行通信其实是用BpBinder，这样一来就进行了隔离！你实例化出你想要的服务端的INTERFACE，然后把BpBinder设置好（把Handler值设置好），通信层就交给BpBinder了！！！所以不管你找的是ServiceManager，还是其他服务端，通信层真正不同的就是Handler的值！！！
+ 现在留下一个问题：BBinder是负责什么的？

我们先理清IServiceManager和BpBInder的关系，所以下面我们就需要开始分析BpServiceManager了,因为我们拿到的IServiceManager其实是BpServiceManager，前面的一系列代码只是去实例化BpServiceManager，然后转为IServiceManager，所以我们要看看BpServiceManager的构造函数

~~~c
//media/ba/sd/aosp/frameworks/native/libs/binder/IServiceManager.cpp
class BpServiceManager : public BpInterface<IServiceManager>
{
public:
    BpServiceManager(const sp<IBinder>& impl)
        : BpInterface<IServiceManager>(impl)
    {
    }

	//可以看到这些都是ServiceManager的方法，他们的具体实现都是由mRemote对象实现的，下面会说mRemote是什么
   ...
    virtual status_t addService(const String16& name, const sp<IBinder>& service,
            bool allowIsolated)
    {
        Parcel data, reply;
        data.writeInterfaceToken(IServiceManager::getInterfaceDescriptor());
        data.writeString16(name);
        data.writeStrongBinder(service);
        data.writeInt32(allowIsolated ? 1 : 0);
        status_t err = remote()->transact(ADD_SERVICE_TRANSACTION, data, &reply);
        return err == NO_ERROR ? reply.readExceptionCode() : err;
    }

  ...
        return res;
    }
};
~~~

看看他的父类BpInterface

~~~c
template<typename INTERFACE>

inlineBpInterface<INTERFACE>::BpInterface(const sp<IBinder>& remote)

    :BpRefBase(remote)//基类构造函数

{

}
~~~

嗯。。。mmp。。看看BpRefBase的构造函数。这里先要注意了，这个remote就是我们前面传进来的BpBinder

~~~c
BpRefBase::BpRefBase(const sp<IBinder>&o)

  //mRemote最终等于那个new 出来的BpBinder(0)

    :mRemote(o.get()), mRefs(NULL), mState(0)

{

   extendObjectLifetime(OBJECT_LIFETIME_WEAK);

 

    if(mRemote) {

       mRemote->incStrong(this);          

        mRefs= mRemote->createWeak(this);

    }

}


~~~

他实例化了一个BpBinder并且放在BpServiceManager里面，BpBInder作为通信层，BpServiceManager是不是定义了一堆ServiceManager的方法提供给MediaServer使用，后面我们需要和ServiceManager通信的话，只需要调用IServiceManager的方法就好了，他其实会去调用BpServiceManager的对应方法，这些方法里面都是用BpBInder对象去和BInder驱动进行通信，然后BInder驱动会和真正的ServiceManager对象进行通信:

~~~c
//media/ba/sd/aosp/frameworks/native/libs/binder/IServiceManager.cpp
virtual status_t addService(const String16& name, const sp<IBinder>& service,
            bool allowIsolated)
    {
        Parcel data, reply;
        data.writeInterfaceToken(IServiceManager::getInterfaceDescriptor());
        data.writeString16(name);
        data.writeStrongBinder(service);
        data.writeInt32(allowIsolated ? 1 : 0);
        status_t err = remote()->transact(ADD_SERVICE_TRANSACTION, data, &reply);
        return err == NO_ERROR ? reply.readExceptionCode() : err;
    }
~~~

我们最后看看BpServiceManager的关系图

![image](https://wiki.jikexueyuan.com/project/deep-android-v1/images/chapter6/image003.png)

现在就不难理解了吧！BpBInder才是真正和BInder驱动打交道的东西！！！也就是上面的mRemote，既然我们已经知道了IServiceManager是怎么来的，也知道了我们调用IServiceManager的方法时，其实是通过BpBInder去和BInder驱动打交道，而我们的MediaServer现在万事具备，开始要向通讯录“ServiceManager”进行注册啦！

~~~c

int main(int argc __unused, char **argv __unused)
{
    signal(SIGPIPE, SIG_IGN);

    sp<ProcessState> proc(ProcessState::self());
    sp<IServiceManager> sm(defaultServiceManager());
    ALOGI("ServiceManager: %p", sm.get());
    InitializeIcuOrDie();
    
    //这里！！这里就是去初始化多媒体服务了，这里就会去向ServiceManager注册了！
    MediaPlayerService::instantiate();
    ResourceManagerService::instantiate();
    registerExtensions();
    ProcessState::self()->startThreadPool();
    IPCThreadState::self()->joinThreadPool();
}

~~~

### 标记A

~~~c
///media/ba/sd/aosp/frameworks/av/media/libmediaplayerservice/MediaPlayerService.cpp
void MediaPlayerService::instantiate() {
    defaultServiceManager()->addService(
            String16("media.player"), new MediaPlayerService()); //new MediaPlayerService()，这里很重要！后面会讲到！！标记为A
}

~~~

看看看！！去注册了！defaultServiceManager这个方法前面已经分析过了，如果IServiceManager已经创建了，就不会再被实例化了的，我们现在就要看看addService方法了

~~~c
//media/ba/sd/aosp/frameworks/native/libs/binder/IServiceManager.cpp 
virtual status_t addService(const String16& name, const sp<IBinder>& service,
            bool allowIsolated)
    {
        Parcel data, reply;
        data.writeInterfaceToken(IServiceManager::getInterfaceDescriptor());
        data.writeString16(name);
        data.writeStrongBinder(service);
        data.writeInt32(allowIsolated ? 1 : 0);
        status_t err = remote()->transact(ADD_SERVICE_TRANSACTION, data, &reply);
        return err == NO_ERROR ? reply.readExceptionCode() : err;
    }
~~~

这里有了两个个Parcel对象，就是数据包！他就像网络请求一样，把请求参数放在data里面，然后还给了个reply作为返回数据包，可以看到他还有token，这个Token里面的内容是：android.os.IServiceManager，name的内容是："media.player"，还把一个MediaPlayerService实例写进了数据包。我们现在看看BpBInder到底做了什么！！

~~~c
//media/ba/sd/aosp/frameworks/native/libs/binder/BpBinder.cpp
status_t BpBinder::transact(
    uint32_t code, const Parcel& data, Parcel* reply, uint32_t flags)
{
    // Once a binder has died, it will never come back to life.
    if (mAlive) {
        status_t status = IPCThreadState::self()->transact(
            mHandle, code, data, reply, flags);
        if (status == DEAD_OBJECT) mAlive = 0;
        return status;
    }

    return DEAD_OBJECT;
}
~~~

可以看到他把任务交给了IPCThreadState去做了，原来BpBinder只是个幌子！！mHandle是0，就是前面传的handle值。我们先了解一下这个IPCThreadState是什么，每个线程都会有一个IPCThreadState，这个IPCThreadState才是真正和BInder驱动进交流的东西，怎么实现每个线程都有自己的IPCThreadState？我已经不想再讲了，翻翻安卓消息机制的代码吧！我们直接看他的transact方法

~~~c
status_t IPCThreadState::transact(int32_t handle,
                                  uint32_t code, const Parcel& data,
                                  Parcel* reply, uint32_t flags)
{
    status_t err = data.errorCheck();

    flags |= TF_ACCEPT_FDS;
...
   
    
    if (err == NO_ERROR) {
        LOG_ONEWAY(">>>> SEND from pid %d uid %d %s", getpid(), getuid(),
            (flags & TF_ONE_WAY) == 0 ? "READ REPLY" : "ONE WAY");
        //这里把数据写进请求？
        err = writeTransactionData(BC_TRANSACTION, flags, handle, code, data, NULL);
    }
    
    if ((flags & TF_ONE_WAY) == 0) {
     ...
    } else {
         //看这里！！发送完请求后就等待驱动回复？
        err = waitForResponse(NULL, NULL);
    }
   
    return err;
}
~~~

就很奇怪，不是已经把数据打包好了？怎么又去打包数据？

~~~c
status_t IPCThreadState::writeTransactionData(int32_t cmd, uint32_t binderFlags,
    int32_t handle, uint32_t code, const Parcel& data, status_t* statusBuffer)
{
    binder_transaction_data tr;

    tr.target.ptr = 0; /* Don't pass uninitialized stack data to a remote process */
    tr.target.handle = handle;
    tr.code = code;
    tr.flags = binderFlags;
    tr.cookie = 0;
    tr.sender_pid = 0;
    tr.sender_euid = 0;
    
    const status_t err = data.errorCheck();
    if (err == NO_ERROR) {
        tr.data_size = data.ipcDataSize();
        tr.data.ptr.buffer = data.ipcData();
        tr.offsets_size = data.ipcObjectsCount()*sizeof(binder_size_t);
        tr.data.ptr.offsets = data.ipcObjects();
    } else if (statusBuffer) {
        tr.flags |= TF_STATUS_CODE;
        *statusBuffer = err;
        tr.data_size = sizeof(status_t);
        tr.data.ptr.buffer = reinterpret_cast<uintptr_t>(statusBuffer);
        tr.offsets_size = 0;
        tr.data.ptr.offsets = 0;
    } else {
        return (mLastError = err);
    }
    
    //最后写到了这里
    mOut.writeInt32(cmd);
    mOut.write(&tr, sizeof(tr));
    
    return NO_ERROR;
}
~~~

所以我们要看看这个mOut是什么东西

~~~c
///media/ba/sd/aosp/frameworks/native/include/binder/IPCThreadState.h            
Parcel              mIn;
Parcel              mOut;
~~~

也是一个数据包，所以上面就是把我们调用的方法参数，服务描述，Handle什么的设置进这个数据包，可以看到还有一个mIn，就是读入的意思，那现在数据准备好了，我们看看waitForResponse方法

~~~c
status_t IPCThreadState::waitForResponse(Parcel *reply, status_t *acquireResult)
{
    uint32_t cmd;
    int32_t err;

    while (1) {
        //这个talkWithDriver就很明显了！！！他就是去和BInder驱动通信了！！我们
        if ((err=talkWithDriver()) < NO_ERROR) break;
        //可以看到果然是从mIn这个数据包读结果
        err = mIn.errorCheck();
        if (err < NO_ERROR) break;
        if (mIn.dataAvail() == 0) continue;
        
        cmd = (uint32_t)mIn.readInt32();
        
        IF_LOG_COMMANDS() {
            alog << "Processing waitForResponse Command: "
                << getReturnString(cmd) << endl;
        }

        //根据cmd做处理，
        switch (cmd) {
      	...
            goto finish;

        default:
            //看这里，这个是当收到回复后的事
            err = executeCommand(cmd);
            if (err != NO_ERROR) goto finish;
            break;
        }
    }

 //这个是本次已经通信完成了，结束了！
finish:
    if (err != NO_ERROR) {
        if (acquireResult) *acquireResult = err;
        if (reply) reply->setError(err);
        mLastError = err;
    }
    
    return err;
}
~~~

所以我们现在就要看两个方法：talkWithDriver()，和executeCommand(cmd)，这两个方法分别代表和驱动通信，和通信得到回复后做的事。

~~~c
status_t IPCThreadState::talkWithDriver(bool doReceive)
{
    if (mProcess->mDriverFD <= 0) {
        return -EBADF;
    }
    
    //这个东西是用来给驱动交换数据的
    binder_write_read bwr;
    
    // Is the read buffer empty?
    const bool needRead = mIn.dataPosition() >= mIn.dataSize();
    
    // We don't want to write anything if we are still reading
    // from data left in the input buffer and the caller
    // has requested to read the next data.
    const size_t outAvail = (!doReceive || needRead) ? mOut.dataSize() : 0;
    
    bwr.write_size = outAvail;
    //我们刚刚设置好的数据在这里传进去了
    bwr.write_buffer = (uintptr_t)mOut.data();

    // This is what we'll read.
    if (doReceive && needRead) {
        bwr.read_size = mIn.dataCapacity();
        bwr.read_buffer = (uintptr_t)mIn.data();
    } else {
        bwr.read_size = 0;
        bwr.read_buffer = 0;
    }

    IF_LOG_COMMANDS() {
        TextOutput::Bundle _b(alog);
        if (outAvail != 0) {
            alog << "Sending commands to driver: " << indent;
            const void* cmds = (const void*)bwr.write_buffer;
            const void* end = ((const uint8_t*)cmds)+bwr.write_size;
            alog << HexDump(cmds, bwr.write_size) << endl;
            while (cmds < end) cmds = printCommand(alog, cmds);
            alog << dedent;
        }
        alog << "Size of receive buffer: " << bwr.read_size
            << ", needRead: " << needRead << ", doReceive: " << doReceive << endl;
    }
    
    // Return immediately if there is nothing to do.
    if ((bwr.write_size == 0) && (bwr.read_size == 0)) return NO_ERROR;

    bwr.write_consumed = 0;
    bwr.read_consumed = 0;
    status_t err;
    
   
    do {
        IF_LOG_COMMANDS() {
            alog << "About to read/write, write size = " << mOut.dataSize() << endl;
        }
#if defined(__ANDROID__)
        //看这里，就是和BInder驱动进行通信了！！！可以看到bwr就是我们设置好的数据
        if (ioctl(mProcess->mDriverFD, BINDER_WRITE_READ, &bwr) >= 0)
            err = NO_ERROR;
        else
            err = -errno;
#else
        err = INVALID_OPERATION;
#endif
        if (mProcess->mDriverFD <= 0) {
            err = -EBADF;
        }
        IF_LOG_COMMANDS() {
            alog << "Finished read/write, write size = " << mOut.dataSize() << endl;
        }
    } while (err == -EINTR);

    IF_LOG_COMMANDS() {
        alog << "Our err: " << (void*)(intptr_t)err << ", write consumed: "
            << bwr.write_consumed << " (of " << mOut.dataSize()
                        << "), read consumed: " << bwr.read_consumed << endl;
    }

    if (err >= NO_ERROR) {
        if (bwr.write_consumed > 0) {
            if (bwr.write_consumed < mOut.dataSize())
                mOut.remove(0, bwr.write_consumed);
            else
                mOut.setDataSize(0);
        }
        if (bwr.read_consumed > 0) {
            //然后把他读出来到mIn中
            mIn.setDataSize(bwr.read_consumed);
            mIn.setDataPosition(0);
        }
        IF_LOG_COMMANDS() {
            TextOutput::Bundle _b(alog);
            alog << "Remaining data size: " << mOut.dataSize() << endl;
            alog << "Received commands from driver: " << indent;
            const void* cmds = mIn.data();
            const void* end = mIn.data() + mIn.dataSize();
            alog << HexDump(cmds, mIn.dataSize()) << endl;
            while (cmds < end) cmds = printReturnCommand(alog, cmds);
            alog << dedent;
        }
        return NO_ERROR;
    }
    
    return err;
}
~~~

所以我们会在mIn中获取到BInder驱动给我们的数据，前面的代码可以看到，我们从mIn中读出cmd，然后调用了executeCommand方法，其实就是在做一些处理，比如这次和驱动通信后，我接下需要做什么东西，或者驱动叫我要做什么东西

### 标记B

~~~c

status_t IPCThreadState::executeCommand(int32_t cmd)
{
    BBinder* obj;
    RefBase::weakref_type* refs;
    status_t result = NO_ERROR;
    
    //可以看到和驱动的Ioctl方法是一样的，根据cmd来switch,这里你现在只需知道，他就是根据驱动返回的cmd来做相应处理就好了，这里是和事件驱动有关，比如你请求权限后，不就在onActivityResult方法里面等待回调吗？，下面的代码除了一些service死亡什么的处理，case：BR_TRANSACTION：就是其他客户端来访问MediaServer后需要做的处理了，这就是从通信层到业务层的地方了，但是我们现在在研究业务层到通信层，这个未解之谜这个后面会详细讲，标记为B
    switch ((uint32_t)cmd) {
   ...
   
    return result;
}
~~~

到这里，MediaServer就已经通过Binder驱动和ServiceManager进行了通信，并且完成了注册！！我们再回到MediaServer的main方法，看看注册成功后还有什么操作

~~~c
int main(int argc __unused, char **argv __unused)
{
    signal(SIGPIPE, SIG_IGN);

    sp<ProcessState> proc(ProcessState::self());
    sp<IServiceManager> sm(defaultServiceManager());
    ALOGI("ServiceManager: %p", sm.get());
    InitializeIcuOrDie();
    MediaPlayerService::instantiate();
    ResourceManagerService::instantiate();
    registerExtensions();
    
    //这两个方法！！
    ProcessState::self()->startThreadPool();
    IPCThreadState::self()->joinThreadPool();
}
~~~

把我看蒙了，startThreadPool开启一个线程池是什么意思？

~~~c
void ProcessState::startThreadPool()
{
    AutoMutex _l(mLock);
    if (!mThreadPoolStarted) {
        mThreadPoolStarted = true;
        //下面看这个方法
        spawnPooledThread(true);
    }
}

void ProcessState::spawnPooledThread(bool isMain)
{
    if (mThreadPoolStarted) {
        String8 name = makeBinderThreadName();
        ALOGV("Spawning new pooled thread, name=%s\n", name.string());
        
       //我们下面会去看PoolTHread的构造函数
        sp<Thread> t = new PoolThread(isMain);
        t->run(name.string());
    }
}


class PoolThread : public Thread
{
public:
    PoolThread(bool isMain)
        : mIsMain(isMain)
    {
    }
    
//在讲LInux的多线程时就说过，线程创建后会去调用这个threadLoop方法，可以看到里面为这个线程创建了一个IPCThreadState
protected:
    virtual bool threadLoop()
    {
        IPCThreadState::self()->joinThreadPool(mIsMain);
        return false;
    }
    
    const bool mIsMain;
};
~~~

而我们在前面的main方法中看到的是MediaServer主线程最后也是调用joinThreadPool方法，但是他也创建了一个子线程，也调用joinThreadPool方法，所以我们要看看这个方法有什么用

~~~c
void IPCThreadState::joinThreadPool(bool isMain)
{
    LOG_THREADPOOL("**** THREAD %p (PID %d) IS JOINING THE THREAD POOL\n", (void*)pthread_self(), getpid());

    //注意看这里，如果isMain是true，就会BC_ENTER_LOOPER！！就是会循环处理的意思
    mOut.writeInt32(isMain ? BC_ENTER_LOOPER : BC_REGISTER_LOOPER);
    
    // This thread may have been spawned by a thread that was in the background
    // scheduling group, so first we will make sure it is in the foreground
    // one to avoid performing an initial transaction in the background.
    set_sched_policy(mMyThreadId, SP_FOREGROUND);
        
    status_t result;
    do {
        processPendingDerefs();
        // now get the next command to be processed, waiting if necessary
        //这个方法会去talkWithDriver，这里是不断的循环去和BINder通信
        result = getAndExecuteCommand();

        if (result < NO_ERROR && result != TIMED_OUT && result != -ECONNREFUSED && result != -EBADF) {
            ALOGE("getAndExecuteCommand(fd=%d) returned unexpected error %d, aborting",
                  mProcess->mDriverFD, result);
            abort();
        }
        
        // Let this thread exit the thread pool if it is no longer
        // needed and it is not the main process thread.
        if(result == TIMED_OUT && !isMain) {
            break;
        }
    } while (result != -ECONNREFUSED && result != -EBADF);

    LOG_THREADPOOL("**** THREAD %p (PID %d) IS LEAVING THE THREAD POOL err=%p\n",
        (void*)pthread_self(), getpid(), (void*)result);
    
    mOut.writeInt32(BC_EXIT_LOOPER);

    talkWithDriver(false);
}

status_t IPCThreadState::getAndExecuteCommand()
{
    status_t result;
    int32_t cmd;

  
    //在这里！！！
    result = talkWithDriver();
    if (result >= NO_ERROR) {
      ...

        pthread_mutex_lock(&mProcess->mThreadCountLock);
        mProcess->mExecutingThreadsCount++;
        if (mProcess->mExecutingThreadsCount >= mProcess->mMaxThreads &&
                mProcess->mStarvationStartTimeMs == 0) {
            mProcess->mStarvationStartTimeMs = uptimeMillis();
        }
        pthread_mutex_unlock(&mProcess->mThreadCountLock);

        //去执行对应的操作
        result = executeCommand(cmd);

        pthread_mutex_lock(&mProcess->mThreadCountLock);
        mProcess->mExecutingThreadsCount--;
        if (mProcess->mExecutingThreadsCount < mProcess->mMaxThreads &&
                mProcess->mStarvationStartTimeMs != 0) {
            int64_t starvationTimeMs = uptimeMillis() - mProcess->mStarvationStartTimeMs;
            if (starvationTimeMs > 100) {
                ALOGE("binder thread pool (%zu threads) starved for %" PRId64 " ms",
                      mProcess->mMaxThreads, starvationTimeMs);
            }
            mProcess->mStarvationStartTimeMs = 0;
        }
        pthread_cond_broadcast(&mProcess->mThreadCountDecrement);
        pthread_mutex_unlock(&mProcess->mThreadCountLock);

        // After executing the command, ensure that the thread is returned to the
        // foreground cgroup before rejoining the pool.  The driver takes care of
        // restoring the priority, but doesn't do anything with cgroups so we
        // need to take care of that here in userspace.  Note that we do make
        // sure to go in the foreground after executing a transaction, but
        // there are other callbacks into user code that could have changed
        // our group so we want to make absolutely sure it is put back.
        set_sched_policy(mMyThreadId, SP_FOREGROUND);
    }

    return result;
}
~~~

所以我们总结一下MediaServer在进行了一系列的初始化后，他就去不断循环通信了，这不就是事件驱动型吗！！！这个MediaServer只开了两个线程去loop，这里是因为他提供4个服务，如果他只提供一个或者两个服务，那其实直接一个主线程也是可以胜任的！流程如下：

+ 打开BInder设备

+ 获取IServiceManager实例
+ 用IServiceManager实例和BInder驱动通信进而在ServiceManager中注册自己
+ 实例化自己提供的服务(这点和SystemService是一样的，MediaServer提供了4个Service，我们上面只说了一种而已)
+ 不断的向Binder驱动通信，等待其他进程和自己进行通信（事件驱动型）

# ServiceManager

前面一直在说MediaServer通过Binder驱动向ServiceManager进行了注册，那现在我们就要分析ServiceManager了！他收到了客户端的请求后做了什么处理？

前面我们说了Handle的值为0，就代表是ServiceManager！那么ServiceManager肯定是告诉了Binder驱动，怎么来的?ServiceManager的路径为：aosp/frameworks/native/cmds/servicemanager/service_manager.c

~~~c
int main()
{
    struct binder_state *bs;

    //打开设备
    bs = binder_open(128*1024);
    if (!bs) {
        ALOGE("failed to open binder driver\n");
        return -1;
    }

    //这里的具体操作是把自己的Handle值设置为0
    if (binder_become_context_manager(bs)) {
        ALOGE("cannot become context manager (%s)\n", strerror(errno));
        return -1;
    }
...
  

    //进入循环，注意这里传入的是svcmgr_handler这个函数指针
    binder_loop(bs, svcmgr_handler);

    return 0;
}
~~~

这里就有3个很重要的步骤，我们一个一个来，先看打开设备

~~~c
//media/ba/sd/aosp/frameworks/native/cmds/servicemanager/binder.c
struct binder_state *binder_open(size_t mapsize)
{
    struct binder_state *bs;
    struct binder_version vers;

  ...
    bs->fd = open("/dev/binder", O_RDWR | O_CLOEXEC);
    if (bs->fd < 0) {
        fprintf(stderr,"binder: cannot open device (%s)\n",
                strerror(errno));
        goto fail_open;
    }

    if ((ioctl(bs->fd, BINDER_VERSION, &vers) == -1) ||
        (vers.protocol_version != BINDER_CURRENT_PROTOCOL_VERSION)) {
        fprintf(stderr,
                "binder: kernel driver version (%d) differs from user space version (%d)\n",
                vers.protocol_version, BINDER_CURRENT_PROTOCOL_VERSION);
        goto fail_open;
    }

   
    bs->mapped = mmap(NULL, mapsize, PROT_READ, MAP_PRIVATE, bs->fd, 0);
    if (bs->mapped == MAP_FAILED) {
        fprintf(stderr,"binder: cannot map device (%s)\n",
                strerror(errno));
        goto fail_map;
    }

    return bs;
...
    return NULL;
}
~~~

真的是熟悉到不能再熟悉了！现在到把自己的Handle值设置为0

~~~c
//media/ba/sd/aosp/frameworks/native/cmds/servicemanager/binder.c
int binder_become_context_manager(struct binder_state *bs)
{
    //就直接把0告诉BInder驱动了，具体的代码就要看Binder的驱动源码，现在只需要知道确实是把ServiceManager的电话号码设置为0了呀！
    return ioctl(bs->fd, BINDER_SET_CONTEXT_MGR, 0);
}
~~~

下面我们就看ServiceManager的本职了！在上面的例子中一直都是业务层（MediaServer）到通信层（BpBinder），一直都是在使用BpBInder进行通信，BpBinder是有一个对应的BBinder，BBinder才是服务端业务层的代理者

~~~c
//media/ba/sd/aosp/frameworks/native/cmds/servicemanager/binder.c
void binder_loop(struct binder_state *bs, binder_handler func)
{
    int res;
    struct binder_write_read bwr;
    uint32_t readbuf[32];

    bwr.write_size = 0;
    bwr.write_consumed = 0;
    bwr.write_buffer = 0;

    readbuf[0] = BC_ENTER_LOOPER;
    binder_write(bs, readbuf, sizeof(uint32_t));

    //和我们前面说的一样，事件驱动型
    for (;;) {
        bwr.read_size = sizeof(readbuf);
        bwr.read_consumed = 0;
        bwr.read_buffer = (uintptr_t) readbuf;

        //这里就不断的去联系BInder驱动，有没有人找我呀？？？
        res = ioctl(bs->fd, BINDER_WRITE_READ, &bwr);

        if (res < 0) {
            ALOGE("binder_loop: ioctl failed (%s)\n", strerror(errno));
            break;
        }

        //这里就是收到了请求，然后用func这个函数去做处理,func前面说了是svcmgr_handler函数
        res = binder_parse(bs, 0, (uintptr_t) readbuf, bwr.read_consumed, func);
        if (res == 0) {
            ALOGE("binder_loop: unexpected reply?!\n");
            break;
        }
        if (res < 0) {
            ALOGE("binder_loop: io error %d %s\n", res, strerror(errno));
            break;
        }
    }
}

~~~

再看看关系图

![image](https://wiki.jikexueyuan.com/project/deep-android-v1/images/chapter6/image003.png)

按照关系图，我们应该找BnServiceManager，但是！！！我找不到，在ServiceManager中是没有使用到BBinder的，所以func这个函数指针并不是由BnServiceManager去赋值的，因为BnServiceManager本就不存在，那ServiceManager是怎么处理来自远方的请求呢？func对应的函数是svcmgr_handler，看下面

~~~c
//media/ba/sd/aosp/frameworks/native/cmds/servicemanager/service_manager.c
int svcmgr_handler(struct binder_state *bs,
                   struct binder_transaction_data *txn,
                   struct binder_io *msg,
                   struct binder_io *reply)
{
    struct svcinfo *si;
    uint16_t *s;
    size_t len;
    uint32_t handle;
    uint32_t strict_policy;
    int allow_isolated;

  ...

    //看这里！！这里就是ServiceManager提供的接口啦！！！
    switch(txn->code) {
    case SVC_MGR_GET_SERVICE:
    case SVC_MGR_CHECK_SERVICE:
        s = bio_get_string16(msg, &len);
        if (s == NULL) {
            return -1;
        }
          handle = do_find_service(s, len, txn->sender_euid, txn->sender_pid);
        if (!handle)
            break;
        bio_put_ref(reply, handle);
        return 0;

    case SVC_MGR_ADD_SERVICE:
        s = bio_get_string16(msg, &len);
        if (s == NULL) {
            return -1;
        }
        handle = bio_get_ref(msg);
        allow_isolated = bio_get_uint32(msg) ? 1 : 0;
        //看这里！！！下面我们会去看这个方法
        if (do_add_service(bs, s, len, handle, txn->sender_euid,
            allow_isolated, txn->sender_pid))
            return -1;
        break;

    case SVC_MGR_LIST_SERVICES: {
        uint32_t n = bio_get_uint32(msg);

        if (!svc_can_list(txn->sender_pid, txn->sender_euid)) {
            ALOGE("list_service() uid=%d - PERMISSION DENIED\n",
                    txn->sender_euid);
            return -1;
        }
        si = svclist;
        while ((n-- > 0) && si)
            si = si->next;
        if (si) {
            bio_put_string16(reply, si->name);
            return 0;
        }
        return -1;
    }
    default:
        ALOGE("unknown code %d\n", txn->code);
        return -1;
    }

    bio_put_uint32(reply, 0);
    return 0;
}

~~~

~~~c
//media/ba/sd/aosp/frameworks/native/cmds/servicemanager/service_manager.c
int do_add_service(struct binder_state *bs,
                   const uint16_t *s, size_t len,
                   uint32_t handle, uid_t uid, int allow_isolated,
                   pid_t spid)
{
    struct svcinfo *si;

    //ALOGI("add_service('%s',%x,%s) uid=%d\n", str8(s, len), handle,
    //        allow_isolated ? "allow_isolated" : "!allow_isolated", uid);

    if (!handle || (len == 0) || (len > 127))
        return -1;

    //我们先看这个，这个方法是用来判断客户端进程是否可以注册的
    if (!svc_can_register(s, len, spid, uid)) {
        ALOGE("add_service('%s',%x) uid=%d - PERMISSION DENIED\n",
             str8(s, len), handle, uid);
        return -1;
    }

   ...
    return 0;
}
~~~

这里其实就是做了一层权限检查，我们继续看do_add_service更加详细的代码

~~~c
int do_add_service(struct binder_state *bs,uint16_t*s, unsigned len,

                      void *ptr, unsigned uid){

 

...... //接前面的代码

    si =find_svc(s, len);

    if (si) {

        if(si->ptr) {

           return -1;

        }

       si->ptr = ptr;

    } else {

        si =malloc(sizeof(*si) + (len + 1) * sizeof(uint16_t));

        if(!si) {

            return -1;

        }

        //ptr是关键数据，可惜为void*类型。只有分析驱动的实现才能知道它的真实含义了。

       si->ptr = ptr;

       si->len = len;

       memcpy(si->name, s, (len + 1) * sizeof(uint16_t));

       si->name[len] = '\0';

       si->death.func = svcinfo_death;//service退出的通知函数

       si->death.ptr = si;

        //这个svclist是一个list，保存了当前注册到ServiceManager中的信息。

       si->next = svclist;

       svclist = si;

    }

 

   binder_acquire(bs,ptr);

  /*

我们希望当服务进程退出后，ServiceManager能有机会做一些清理工作，例如释放前面malloc出来的si。

binder_link_to_death完成这项工作，每当有服务进程退出时，ServiceManager都会得到来自

Binder设备的通知。

  */

   binder_link_to_death(bs, ptr, &si->death);

    return 0;

}
~~~

##  ServiceManager存在的意义

为何需要一个ServiceManager，其重要作用何在？

·  ServiceManger能集中管理系统内的所有服务，它能施加权限控制，并不是任何进程都能注册服务。

·  ServiceManager支持通过字符串名称来查找对应的Service。这个功能很像DNS。

·  由于各种原因，Server进程可能生死无常。如果让每个Client都去检测，压力实在太大。现在有了统一的管理机构，Client只需要查询ServiceManager，就能把握动向，得到最新信息。这可能正是ServiceManager存在的最大意义吧。

# 解决未解之谜

![image](http://wiki.jikexueyuan.com/project/deep-android-v1/images/chapter6/image004.png)



回顾一下BInder架构的图，我们想在ServiceManager上找到BBinder的实现，但是失败了，ServiceManager并没按照这样的规范来，所以我们找找其他的例子。

假设有一个客户端需要使用到MediaServer的服务，那肯定需要向ServiceManager获取到一个能和MediaServer交互的BpBinder，然后转换为I******Service，就可以和MediaServer的服务进通信了，我们找到了一个例子：IMediaDeathNotifier。这个程序需要用到MediaServer的服务。

~~~c
//media/ba/sd/aosp/frameworks/av/media/libmedia/IMediaDeathNotifier.cpp
IMediaDeathNotifier::getMediaPlayerService()
{
    ALOGV("getMediaPlayerService");
    Mutex::Autolock _l(sServiceLock);
    if (sMediaPlayerService == 0) {
        //获取ServiceManager的代理
        sp<IServiceManager> sm = defaultServiceManager();
        sp<IBinder> binder;
        do {
            //根据字符串去查到要的服务，这里我们要思考一下，我们这里肯定是拿到BpBInder，但是我们拿IServiceManager里面的BpBInder的时候是直接new的！！！而这里是找ServiceManager拿的！！在很前面我们留下一个重要的标记A，就是那种时候服务端new出了一个BpBInder给客户端用！！！！所以这里和ServiceManager是不一样的！原因就是这些普通的服务端没有在BInder驱动注册“电话号码”，只有ServiceManager有电话号码。
            binder = sm->getService(String16("media.player"));
            if (binder != 0) {
                break;
            }
            ALOGW("Media player service not published, waiting...");
            //如果服务未启动就等他启动
            usleep(500000); // 0.5 s
        } while (true);

        if (sDeathNotifier == NULL) {
            sDeathNotifier = new DeathNotifier();
        }
        binder->linkToDeath(sDeathNotifier);
        
        //熟悉的方法，转成IMediaPlayerService，就能愉快的使用了,这里你可能会有些疑问，我们前面标记A处不是传的是MediaPlayerService吗？按照继承图，这个东西是继承于BnMediaPlayerService，但是他们都属于IMediaPlayerService，那怎么就和Bpxxx有关系呢？他们的基类都是想相同的IMediaPlayerService，为了不向客户端暴露Bp的指针，所以客户端都是使用IMediaPlayerService，Bpxxxx和Bnxxxx的具体实现放在了IMediaPlayerService中了
        sMediaPlayerService = interface_cast<IMediaPlayerService>(binder);
    }
    ALOGE_IF(sMediaPlayerService == 0, "no media player service!?");
    return sMediaPlayerService;
}

~~~

我们现在有个问题，明明在注册的时候给ServiceManager的MediaPlayerService是属于Bn类型的，但是客户端在向ServiceManager获取的时候却变成了Bp类型的，就像我们上面注释说的，没有必要向客户端暴露Bp指针，直接用IMediaPlayerService就好了，我们现在要找到ServiceManager的获取服务的过程

~~~c
int svcmgr_handler(struct binder_state *bs,
                   struct binder_transaction_data *txn,
                   struct binder_io *msg,
                   struct binder_io *reply)
{
  ...
    switch(txn->code) {
    case SVC_MGR_GET_SERVICE:
    case SVC_MGR_CHECK_SERVICE:
          s = bio_get_string16(msg, &len);
        if (s == NULL) {
            return -1;
        }
        handle = do_find_service(s, len, txn->sender_euid, txn->sender_pid);
        if (!handle)
            break;
        bio_put_ref(reply, handle);//可以看到直接返回了reply，所以我们要看Bp端的IServiceManager
        return 0;}
~~~

我们找到IServiceManager的getService的具体实现，就是checkService（），但是我们看了一下



~~~c
//media/ba/sd/aosp/frameworks/native/libs/binder/IServiceManager.cpp    
virtual sp<IBinder> checkService( const String16& name) const
    {
        Parcel data, reply;
        data.writeInterfaceToken(IServiceManager::getInterfaceDescriptor());
        data.writeString16(name);
        remote()->transact(CHECK_SERVICE_TRANSACTION, data, &reply);//这里就是刚刚去联系ServiceManager调用getService，最后会到下面，肯定是返回一个Bp类型的MediaPlayerService代理
        return reply.readStrongBinder();
    }
~~~

transact方法就是去和binder通信去联系ServiceManager

所以现在我们看看readStrongBinder

~~~c
//media/ba/sd/aosp/frameworks/native/libs/binder/Parcel.cpp
sp<IBinder> Parcel::readStrongBinder() const
{
    sp<IBinder> val;
    //继续
    readStrongBinder(&val);
    return val;
}


status_t Parcel::readStrongBinder(sp<IBinder>* val) const
{
    //继续
    return unflatten_binder(ProcessState::self(), *this, val);
}

status_t unflatten_binder(const sp<ProcessState>& proc,
    const Parcel& in, sp<IBinder>* out)
{
    const flat_binder_object* flat = in.readObject(false);

    if (flat) {
        switch (flat->type) {
            case BINDER_TYPE_BINDER:
                *out = reinterpret_cast<IBinder*>(flat->cookie);
                return finish_unflatten_binder(NULL, *flat, in);
                
              //看名字就知道Type是BINDER_TYPE_HANDLE，为什么呢？
            case BINDER_TYPE_HANDLE:
                //因为这里！！！看到了getStrongProxyForHandle这个名字！！
                *out = proc->getStrongProxyForHandle(flat->handle);
                return finish_unflatten_binder(
                    static_cast<BpBinder*>(out->get()), *flat, in);
        }
    }
    return BAD_TYPE;
}


~~~

~~~c
sp<IBinder> ProcessState::getStrongProxyForHandle(int32_t handle)
{
	sp<IBinder> result;
	
	handle_entry* e = lookupHandleLocked(handle);
	if(e != NULL) {
		IBinder* b = e->binder;
		if(b == NULL || !e->refs->attemptIncWeak(this)) {
            //啊！！！找到了！！
			b = new BpBinder(handle);
			e->binder = b;
			if( b ) e->refs = e->getWeakRefs();
			result = b;
		} else {
			result.force_set(b);
			e->refs->decWeak(this);
		}
	}
	return result;

~~~

原来是ProcessState提供的方法！真的是气啊，找了半天！最后会Bp转成IMediaPlayerService给IMediaDeathNotifier客户端使用！然后IMediaDeathNotifier使用IMediaPlayerService的接口时，MediaServer会怎么处理呢？前面已经讲过了，在IPCThreadState::executeCommand方法处理,也就是我们留下问题的地方：标记B！！注意！现在下面代码运行在的进程不是IMediaDeathNotifier，而是MediaServer进程了！！因为你使用IMediaPlayerService接口是进行了PC的哦！！情景要搞清楚！！

~~~c
status_t IPCThreadState::executeCommand(int32_t cmd)
{
    BBinder* obj;
    RefBase::weakref_type* refs;
    status_t result = NO_ERROR;
    
    switch ((uint32_t)cmd) {
 ...
    //就是这里，我们这一次肯定能看到BBInder
    case BR_TRANSACTION:
        {
        ...
            if (tr.target.ptr) {
                // We only have a weak reference on the target object, so we must first try to
                // safely acquire a strong reference before doing anything else with it.
                if (reinterpret_cast<RefBase::weakref_type*>(
                        tr.target.ptr)->attemptIncStrong(this)) {
                    //啊！！！！找到BBinder了！！下面肯定就是调用transact方法
                    error = reinterpret_cast<BBinder*>(tr.cookie)->transact(tr.code, buffer,
                            &reply, tr.flags);
         ...

    return result;
}
~~~

这里就是前面说的客户端的BpBinder对应的是服务端的BBinder啦！剩下就是看看通信层到底怎么跑到业务层了！！所以肯定是IMediaPlayerService子类的onTransact方法，我们先看IMediaPalyerService

~~~c
//media/ba/sd/aosp/frameworks/av/media/libmedia/IMediaPlayerService.cpp
status_t BnMediaPlayerService::onTransact(
    uint32_t code, const Parcel& data, Parcel* reply, uint32_t flags)
{
    switch (code) {
            ...
       //随便看两个就好了
        case CREATE_MEDIA_RECORDER: {
            CHECK_INTERFACE(IMediaPlayerService, data, reply);
            const String16 opPackageName = data.readString16();
            //看这里！createMediaRecorder方法的具体实现在哪里呢？在标记A中我们直接传的是XXXService，这里对应的肯定是"MediaPlayerService"，而MediaPlayerService就是具体的服务，所以我们只需要确认 "MediaPlayerService"是否继承于BnMediaPlayerService就好了，

            sp<IMediaRecorder> recorder = createMediaRecorder(opPackageName);
            reply->writeStrongBinder(IInterface::asBinder(recorder));
            return NO_ERROR;
        } break;
        case CREATE_METADATA_RETRIEVER: {
            CHECK_INTERFACE(IMediaPlayerService, data, reply);
            sp<IMediaMetadataRetriever> retriever = createMetadataRetriever();
            reply->writeStrongBinder(IInterface::asBinder(retriever));
            return NO_ERROR;
        } break;
     
        
    }
}
~~~

我们看看

~~~c
//media/ba/sd/aosp/frameworks/av/media/libmediaplayerservice/MediaPlayerService.h
class MediaPlayerService : public BnMediaPlayerService
{}
~~~

果不其然吧！！！其实根本不用想，都在自己的进程了，前面的BBinder肯定是MediaPlayerService是的实例。

## 小结

在上面的代码中生成BpBInder的方式有两种！转换成BpBInder的方式有一种，ServiceManager是没有Bnxxx的

**ServiceManager**

+ 这里面是因为BInder驱动和大家都知道Handle为0就是ServiceManager！
+ 所以ServiceManager的Bp是在defaultServiceManager方法里面直接new的！！
+ ServiceManager是没有BnServiceManager的，这应该和他的一些其他功能有关，我还暂时不知道

**OtherService**

+ 这些服务都会在ServiceManager注册，我们能看到他是直接把IMediaPlayerService的实例给ServiceManager
+ 其他客户端会从ServiceManager取到IMediaPlayerService，需要注意的是这个类型是Bn也就是BBinder类型的，在这个过程中会调用ProcessState::getStrongProxyForHandle方法生成对应的Bp，放在刚刚获取到的xxxxservice里面，然后会经过interface_case进行转换为IMediaPlayerService，进而得到BpBInder。
+ 这些个服务基本都有Bnxxxx！Bn就是具体的实例

所以有意思的就是业务层和通信层的划分和相连，BInder架构的核心部分是非常好理解的！经过上面的学习，我们就明白是可以不经过ServiceManager的！只需要按照下面的架构走就对了

![image](http://wiki.jikexueyuan.com/project/deep-android-v1/images/chapter6/image004.png)



# Binder与多线程的关系

以MS为例，现在程序运行正常。此时MS：

（1）通过startThreadPool启动了一个线程，这个线程在talkWithDriver。

（2）主线程通过joinThreadPool，也在talkWithDriver。

至此我们已知道，有两个线程在和Binder设备打交道。这时在业务逻辑上需要与ServiceManager交互，比如要调用listServices打印所有服务的名字，假设这是MS中的第三个线程。按照之前的分析，它最终会调用IPCThreadState的transact函数，这个函数会talkWithDriver并把请求发到ServiceManager进程，然后等待来自Binder设备的回复。那么现在一共有三个线程（不论是在等待来自其他Client的请求，还是在等待listService的回复）都在talkWithDriver。

ServiceManager处理完了listServices，把回复结果写回Binder驱动，那么，MS中哪个线程会收到回复呢？此问题如图6-6表示：

![image](http://wiki.jikexueyuan.com/project/deep-android-v1/images/chapter6/image006.png)

图6-6 本问题的示意图

显而易见，当然是调用listServices的那个线程会得到结果。为什么？因为如果不这么做，则会导致下面情况的发生：

·  如果是没有调用listServices的线程1或者线程2得到回复，那么它们应该唤醒调用listServices的线程3。因为这时已经有了结果，线程3应该从listServices函数调用中返回。

·  这其中的线程等待、唤醒、切换会浪费不少宝贵的时间片，而且代码逻辑会极其复杂。

看来，Binder设备把发起请求的线程牢牢地拴住了，必须收到回复才能放它离开。这种一一对应的方式极大简化了代码层的处理逻辑。

# 有人情味的讣告

在socket编程中，当一个socket 关闭后，我们无比希望另一端的select/poll/epoll/WaitForXXX有相应返回，以示通知。

说明：在广域网中，我们常常会因为收不到或者延时收到socket的close消息而烦恼。

在Binder系统中，要是明确表示对BnXXX的生死非常关心，那么在它离世后你会收到一份讣告。你可以嚎啕大哭，或者什么也不做。

关于这个问题，请直接看源码中的例子吧。

## 1. 表达你的关心

要想收到讣告，必须先要表达你的关心：做下面两件事：

·  从IBinder::DeathRecipient派生一个类，并实现其中的通知函数binderDied。这个函数一旦被调用，就相当于你收到了讣告。

6.5.2  ·  把这个类注册到系统，告诉你关心哪一个BnXXX的生死。

看示例代码，它在MediaMetadataRetriever.cpp中，如下所示：

[-->MediaMetadataRetriever.cpp]

const sp<IMediaPlayerService>&MediaMetadataRetriever::getService()

{

   Mutex::Autolock lock(sServiceLock);

​    if(sService.get() == 0) {

​       sp<IServiceManager> sm = defaultServiceManager();

​       sp<IBinder> binder;

​        do {

​           binder = sm->getService(String16("media.player"));

​           if (binder != 0) {

​               break;

​            }

​            usleep(500000); // 0.5 s

​        }while(true);

​        if(sDeathNotifier == NULL) {

​           sDeathNotifier = new DeathNotifier();

​        }

​       //调用下面这个函数，告诉系统我们对这个binder的生死有兴趣

​      //这个binder是一个BpBinder，它关心的是对端BBinder，也即是BnXXX的父类。

​        binder->linkToDeath(sDeathNotifier);

​       sService = interface_cast<IMediaPlayerService>(binder);

​    }

​    returnsService;

}

## 2. 讣告是怎么收到的？

那么，这份讣告是怎么收到的呢？答案也在executeCommand中，代码如下所示：

[-->IPCThreadState.cpp]

status_t IPCThreadState::executeCommand(int32_t cmd)

{

​    BBinder*obj;

   RefBase::weakref_type* refs;

​    status_tresult = NO_ERROR;

   

​    switch(cmd) {

​    caseBR_ERROR:

​       result = mIn.readInt32();

​       break;

​        ......

​      caseBR_DEAD_BINDER:

​        {

​           //Binder驱动会通知死亡消息。下面的proxy对应着已经死亡的远端BBinder。

​           BpBinder *proxy =(BpBinder*)mIn.readInt32();

​            //发送讣告，Obituary是讣告的意思。最终会传递到你的DeathNotifier中。

​           proxy->sendObituary();

​           mOut.writeInt32(BC_DEAD_BINDER_DONE);

​           mOut.writeInt32((int32_t)proxy);

​        }break;

​      default:

​        result = UNKNOWN_ERROR;

​       break;

​    }

## 3. 你死了，我怎么办？

收到讣告后该怎么办呢？有一些代码看起来非常薄情寡义，如下所示：

[-->MediaMetadataRetriever.cpp]

/*

  DeathNotifier是MediaMetadataRetriever的内部类，前面在getService函数中

  我们注册了它对BnMediaPlayerService的关心。

*/

voidMediaMetadataRetriever::DeathNotifier::binderDied(const wp<IBinder>&who) {

   Mutex::Autolock lock(MediaMetadataRetriever::sServiceLock);

  //把自己保存的BpMediaPlayerService对象干掉！

MediaMetadataRetriever::sService.clear();

  LOGW("MediaMetadataRetriever serverdied!");//打印一下LOG，这样就完事大吉了。

}

## 4. 承受不住的诺言

我答应收到讣告后给你送终，可是如果我要是死在你前面或者中途我不想接收讣告，又该怎么办呢？先来看下面的代码：

[-->MediaMetadataRetriever.cpp]

MediaMetadataRetriever::DeathNotifier::~DeathNotifier()

{

   Mutex::Autolocklock(sServiceLock);

// DeathNotifier对象不想活了，但是BnMediaPlayerService还活着，

// 或者DeathNotifier中途变卦。怎么办？

//unlinkToDeath调用可以取消对BnMediaPlayerService的关心。

​    if(sService != 0) {

​       sService->asBinder()->unlinkToDeath(this);

​    }

}

Binder的这个讣告是不是很有人情味呢？想知道它是怎么做到的吗？还是先去看看驱动的实现吧。

# 匿名Service

匿名Service就是没有注册的Service，这句话是什么意思？

·  没有注册意味着这个Service没有在ServiceManager上注册。

·  它是一个Service又表示它确实是一个基于Binder通讯的C/S结构。

再看下面的代码，或许就会明白是什么意思了。

[-->IMediaPlayerService.cpp]

status_t BnMediaPlayerService::onTransact(uint32_tcode, const Parcel& data,

​                                                  Parcel* reply, uint32_t flags)

{

   switch(code) {

​        caseCREATE_URL: {

​           CHECK_INTERFACE(IMediaPlayerService, data, reply);

​             ...

​            //player是一个IMediaPlayer类型的对象

​            sp<IMediaPlayer> player =create(

​                   pid, client, url, numHeaders > 0 ? &headers : NULL);

​            //下面这句话也很重要

​           reply->writeStrongBinder(player->asBinder());

​           return NO_ERROR;

​        }break;

当MediaPlayerClient调用create函数时，MediaPlayerService会返回一个IMediaPlayer对象，此后，MediaPlayerClient即可直接使用这个IMediaPlayer来进行跨进程的函数调用了。

请看，这里确实也存在C/S的两端：

·  BpMediaPlayer，由MediaPlayerClient使用，它用来调用IMediaPlayer提供的业务服务

·  BnMediaPlayer，由MediaPlayerService使用，用来处理来自Client端的业务请求。

上面明显是一个C/S结构，但在ServiceManager中，肯定没有IMediaPlayer的信息，那么BpMediaPlayer是如何得到BnMediaPlayer的handle值的呢？

注意：handle事关通信的目的端，因此它非常重要。

答案可能就在下面这句话中：

reply->writeStrongBinder(player->asBinder());//将Binder类型作为一种特殊数据类型处理

当这个reply写到Binder驱动中时，驱动可能会特殊处理这种IBinder类型的数据，例如为这个BBinder建立一个独一无二的handle，这其实相当于在Binder驱动中注册了一项服务。

通过这种方式，MS输出了大量的Service，例如IMediaPlayer和IMediaRecorder等。

说明：关于这个问题，也可以查看驱动的实现来验证这一想法。

## 终于结束了！！

关系非常的复杂，第一次写这么长的笔记，难受。